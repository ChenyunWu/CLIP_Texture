{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feca272-e8ec-4978-902c-958676a6907b",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# os.chdir('/your/path/to/Clip')\n",
    "!pwd\n",
    "!echo $CUDA_VISIBLE_DEVICES\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a904f1f0-bc8e-483d-b80c-e8355f9675a5",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextureDescriptionData ready. \n",
      "655 phrases with frequency above 10.\n",
      "Image count: train 3222, val 805, test 1342\n",
      "dataset ready.\n"
     ]
    }
   ],
   "source": [
    "from dtd2.data_api.dataset_api import TextureDescriptionData\n",
    "from dtd2.models.layers.util import print_tensor_stats\n",
    "from dtd2.data_api.eval_retrieve import retrieve_eval as dtd2_eval\n",
    "\n",
    "split = 'test'\n",
    "dataset = TextureDescriptionData(phid_format=None)\n",
    "print('dataset ready.')\n",
    "img_num = len(dataset.img_splits[split])\n",
    "phrase_num = len(dataset.phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d7e011b-8fc4-4d77-8145-405a3f4e2765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14797\n"
     ]
    }
   ],
   "source": [
    "desc_num = 0\n",
    "for img_name in dataset.img_splits['train']:\n",
    "    desc_num += len(dataset.img_data_dict[img_name]['descriptions'])\n",
    "print(desc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fadc34a7-3db2-43b5-a487-6c4c6d397b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [os.path.join('data/DTD2/images', img_name) for img_name in dataset.img_splits[split]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911dd554-c320-424e-83bd-72a0ad17db6d",
   "metadata": {},
   "source": [
    "# Specialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f01e847-2615-4e19-9106-2f6808b2ee66",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from utils.dtd2_triplet_encoder import TripletEncoder\n",
    "dtd2_encoder = TripletEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aaf8ba7-0ba5-496f-8150-1ba72596ba92",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 imgs encoded\n",
      "100 imgs encoded\n",
      "200 imgs encoded\n",
      "300 imgs encoded\n",
      "400 imgs encoded\n",
      "500 imgs encoded\n",
      "600 imgs encoded\n",
      "700 imgs encoded\n",
      "800 imgs encoded\n",
      "900 imgs encoded\n",
      "1000 imgs encoded\n",
      "1100 imgs encoded\n",
      "1200 imgs encoded\n",
      "1300 imgs encoded\n",
      "1342 imgs encoded\n",
      "torch.Size([1342, 256])\n"
     ]
    }
   ],
   "source": [
    "img_vecs = dtd2_encoder.encode_imgs(img_paths)\n",
    "print(img_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f599571-6f2d-4bef-88cf-fa0b3bc94543",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527 remaining texts\n",
      "399 remaining texts\n",
      "271 remaining texts\n",
      "143 remaining texts\n",
      "15 remaining texts\n",
      "torch.Size([655, 256])\n"
     ]
    }
   ],
   "source": [
    "phrase_vecs = dtd2_encoder.encode_text_list(dataset.phrases)\n",
    "print(phrase_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b64bdab-a57c-4d37-a9aa-b7f8c053c962",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1342, 256])\n",
      "torch.Size([655, 256])\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "STAT pred_scores: shape torch.Size([1342, 655]) device cpu; mean -12.123; min -24.844; max -4.927; std 1.761\n"
     ]
    }
   ],
   "source": [
    "print(img_vecs.shape)\n",
    "print(phrase_vecs.shape)\n",
    "\n",
    "neg_distances = torch.zeros((img_num, phrase_num))\n",
    "with torch.no_grad():\n",
    "    for img_i in range(img_num):\n",
    "        for ph_i in range(phrase_num):\n",
    "            v1 = img_vecs[img_i]\n",
    "            v2 = phrase_vecs[ph_i]\n",
    "            neg_distances[img_i, ph_i] = - dtd2_encoder.dist(v1, v2)\n",
    "        if img_i % 100 == 0:\n",
    "            print(img_i)\n",
    "\n",
    "print_tensor_stats(neg_distances, 'pred_scores')\n",
    "mdtd2_scores = neg_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e360da71-83de-457a-aa80-c5a9395694bc",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/work1/elm/chenyun/Clip/dependencies/DescribingTextures/data_api/utils/retrieval_metrics.py:146: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = np.nan_to_num(pred_num * 1.0 / gt_count)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## retrieve_eval phrase2img on test ##\n",
      "mean_average_precision: 0.1350\n",
      "mean_reciprocal_rank: 0.3112\n",
      "precision_at_005: 0.1652\n",
      "precision_at_010: 0.1565\n",
      "precision_at_020: 0.1457\n",
      "precision_at_050: 0.1162\n",
      "precision_at_100: 0.0885\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0524\n",
      "recall_at_010: 0.0978\n",
      "recall_at_020: 0.1732\n",
      "recall_at_050: 0.3362\n",
      "recall_at_100: 0.4738\n",
      "## retrieve_eval img2phrase on test ##\n",
      "mean_average_precision: 0.3177\n",
      "mean_reciprocal_rank: 0.7412\n",
      "precision_at_005: 0.4170\n",
      "precision_at_010: 0.3256\n",
      "precision_at_020: 0.2360\n",
      "precision_at_050: 0.1375\n",
      "precision_at_100: 0.0847\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.2017\n",
      "recall_at_010: 0.3135\n",
      "recall_at_020: 0.4504\n",
      "recall_at_050: 0.6488\n",
      "recall_at_100: 0.7934\n"
     ]
    }
   ],
   "source": [
    "p2i_result = dtd2_eval(mode='phrase2img', match_scores=mdtd2_scores, dataset=dataset,\n",
    "                           split=split, visualize_path='output/dtd2_model_result')\n",
    "\n",
    "i2p_result = dtd2_eval(mode='img2phrase', match_scores=mdtd2_scores, dataset=dataset,\n",
    "                           split=split, visualize_path='output/dtd2_model_result')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7865630",
   "metadata": {},
   "source": [
    "# CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53338393-844e-42fb-bef4-8adab100d001",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClipEncoder ready.\n"
     ]
    }
   ],
   "source": [
    "from utils.clip_encoder import ClipEncoder\n",
    "clip_encoder = ClipEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95c635c2-d700-449b-a090-98360c66c9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1214 remaining imgs\n",
      "1086 remaining imgs\n",
      "958 remaining imgs\n",
      "830 remaining imgs\n",
      "702 remaining imgs\n",
      "574 remaining imgs\n",
      "446 remaining imgs\n",
      "318 remaining imgs\n",
      "190 remaining imgs\n",
      "62 remaining imgs\n",
      "images encoded\n"
     ]
    }
   ],
   "source": [
    "img_vecs = clip_encoder.encode_imgs(img_paths)\n",
    "print('images encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d41fc026-0059-49e5-8747-988fb21d1178",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527 remaining texts\n",
      "399 remaining texts\n",
      "271 remaining texts\n",
      "143 remaining texts\n",
      "15 remaining texts\n",
      "phrases encoded\n",
      "torch.Size([655, 512])\n"
     ]
    }
   ],
   "source": [
    "template = 'An image of %s texture'\n",
    "phrases = [template % p for p in dataset.phrases]\n",
    "phrase_vecs = clip_encoder.encode_text_list(phrases)\n",
    "print('phrases encoded')\n",
    "print(phrase_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d007108e-08b6-4d9d-aa51-192b0feaffc5",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAT clip_scores: shape torch.Size([1342, 655]) device cpu; mean 0.002; min 0.001; max 0.002; std 0.000\n"
     ]
    }
   ],
   "source": [
    "clip_scores = img_vecs @ phrase_vecs.T\n",
    "clip_scores = clip_scores.float().softmax(dim=-1).cpu()\n",
    "\n",
    "print_tensor_stats(clip_scores, 'clip_scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9ad3bad-e321-46f4-ace2-e423ebf96fcb",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## retrieve_eval phrase2img on test ##\n",
      "mean_average_precision: 0.1274\n",
      "mean_reciprocal_rank: 0.3215\n",
      "precision_at_005: 0.1695\n",
      "precision_at_010: 0.1536\n",
      "precision_at_020: 0.1322\n",
      "precision_at_050: 0.1005\n",
      "precision_at_100: 0.0762\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0616\n",
      "recall_at_010: 0.1077\n",
      "recall_at_020: 0.1731\n",
      "recall_at_050: 0.2982\n",
      "recall_at_100: 0.4225\n",
      "## retrieve_eval img2phrase on test ##\n",
      "mean_average_precision: 0.1220\n",
      "mean_reciprocal_rank: 0.4006\n",
      "precision_at_005: 0.1768\n",
      "precision_at_010: 0.1473\n",
      "precision_at_020: 0.1142\n",
      "precision_at_050: 0.0736\n",
      "precision_at_100: 0.0519\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0842\n",
      "recall_at_010: 0.1397\n",
      "recall_at_020: 0.2154\n",
      "recall_at_050: 0.3483\n",
      "recall_at_100: 0.4889\n"
     ]
    }
   ],
   "source": [
    "clip_p2i = dtd2_eval(mode='phrase2img', match_scores=clip_scores, dataset=dataset,\n",
    "                     split=split, visualize_path='output/dtd2_p2i_result')\n",
    "\n",
    "clip_i2p = dtd2_eval(mode='img2phrase', match_scores=clip_scores, dataset=dataset,\n",
    "                     split=split, visualize_path='output/dtd2_p2i_result')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1a39c-f944-4145-acda-41994b9909bd",
   "metadata": {},
   "source": [
    "# CLIPP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97315a3-f0b1-4d14-8cdd-233c98188558",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch130\n",
      "ClipEncoder ready.\n",
      "CLIPP ready\n",
      "1214 remaining imgs\n",
      "1086 remaining imgs\n",
      "958 remaining imgs\n",
      "830 remaining imgs\n",
      "702 remaining imgs\n",
      "574 remaining imgs\n",
      "446 remaining imgs\n",
      "318 remaining imgs\n",
      "190 remaining imgs\n",
      "62 remaining imgs\n",
      "images encoded\n",
      "527 remaining texts\n",
      "399 remaining texts\n",
      "271 remaining texts\n",
      "143 remaining texts\n",
      "15 remaining texts\n",
      "527 remaining texts\n",
      "399 remaining texts\n",
      "271 remaining texts\n",
      "143 remaining texts\n",
      "15 remaining texts\n",
      "phrases encoded\n",
      "torch.Size([655, 512])\n",
      "STAT clipp_scores: shape torch.Size([1342, 655]) device cpu; mean 0.002; min 0.000; max 0.307; std 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/work1/elm/chenyun/Clip/dependencies/DescribingTextures/data_api/utils/retrieval_metrics.py:146: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = np.nan_to_num(pred_num * 1.0 / gt_count)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## retrieve_eval phrase2img on test ##\n",
      "mean_average_precision: 0.1372\n",
      "mean_reciprocal_rank: 0.3248\n",
      "precision_at_005: 0.1795\n",
      "precision_at_010: 0.1685\n",
      "precision_at_020: 0.1490\n",
      "precision_at_050: 0.1122\n",
      "precision_at_100: 0.0833\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0617\n",
      "recall_at_010: 0.1122\n",
      "recall_at_020: 0.1849\n",
      "recall_at_050: 0.3198\n",
      "recall_at_100: 0.4398\n",
      "## retrieve_eval img2phrase on test ##\n",
      "mean_average_precision: 0.1189\n",
      "mean_reciprocal_rank: 0.3589\n",
      "precision_at_005: 0.1610\n",
      "precision_at_010: 0.1362\n",
      "precision_at_020: 0.1121\n",
      "precision_at_050: 0.0776\n",
      "precision_at_100: 0.0549\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0770\n",
      "recall_at_010: 0.1297\n",
      "recall_at_020: 0.2114\n",
      "recall_at_050: 0.3671\n",
      "recall_at_100: 0.5150\n",
      "\n",
      "epoch120\n",
      "ClipEncoder ready.\n",
      "CLIPP ready\n",
      "1214 remaining imgs\n",
      "1086 remaining imgs\n",
      "958 remaining imgs\n",
      "830 remaining imgs\n",
      "702 remaining imgs\n",
      "574 remaining imgs\n",
      "446 remaining imgs\n",
      "318 remaining imgs\n",
      "190 remaining imgs\n",
      "62 remaining imgs\n",
      "images encoded\n",
      "527 remaining texts\n",
      "399 remaining texts\n",
      "271 remaining texts\n",
      "143 remaining texts\n",
      "15 remaining texts\n",
      "527 remaining texts\n",
      "399 remaining texts\n",
      "271 remaining texts\n",
      "143 remaining texts\n",
      "15 remaining texts\n",
      "phrases encoded\n",
      "torch.Size([655, 512])\n",
      "STAT clipp_scores: shape torch.Size([1342, 655]) device cpu; mean 0.002; min 0.000; max 0.279; std 0.004\n",
      "## retrieve_eval phrase2img on test ##\n",
      "mean_average_precision: 0.1396\n",
      "mean_reciprocal_rank: 0.3184\n",
      "precision_at_005: 0.1832\n",
      "precision_at_010: 0.1684\n",
      "precision_at_020: 0.1503\n",
      "precision_at_050: 0.1134\n",
      "precision_at_100: 0.0838\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0652\n",
      "recall_at_010: 0.1125\n",
      "recall_at_020: 0.1915\n",
      "recall_at_050: 0.3229\n",
      "recall_at_100: 0.4450\n",
      "## retrieve_eval img2phrase on test ##\n",
      "mean_average_precision: 0.1218\n",
      "mean_reciprocal_rank: 0.3619\n",
      "precision_at_005: 0.1641\n",
      "precision_at_010: 0.1422\n",
      "precision_at_020: 0.1166\n",
      "precision_at_050: 0.0785\n",
      "precision_at_100: 0.0556\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0786\n",
      "recall_at_010: 0.1342\n",
      "recall_at_020: 0.2212\n",
      "recall_at_050: 0.3706\n",
      "recall_at_100: 0.5228\n",
      "\n",
      "epoch110\n",
      "ClipEncoder ready.\n",
      "CLIPP ready\n",
      "1214 remaining imgs\n",
      "1086 remaining imgs\n",
      "958 remaining imgs\n",
      "830 remaining imgs\n",
      "702 remaining imgs\n",
      "574 remaining imgs\n",
      "446 remaining imgs\n",
      "318 remaining imgs\n",
      "190 remaining imgs\n",
      "62 remaining imgs\n",
      "images encoded\n",
      "527 remaining texts\n",
      "399 remaining texts\n",
      "271 remaining texts\n",
      "143 remaining texts\n",
      "15 remaining texts\n",
      "527 remaining texts\n",
      "399 remaining texts\n",
      "271 remaining texts\n",
      "143 remaining texts\n",
      "15 remaining texts\n",
      "phrases encoded\n",
      "torch.Size([655, 512])\n",
      "STAT clipp_scores: shape torch.Size([1342, 655]) device cpu; mean 0.002; min 0.000; max 0.275; std 0.003\n",
      "## retrieve_eval phrase2img on test ##\n",
      "mean_average_precision: 0.1390\n",
      "mean_reciprocal_rank: 0.3136\n",
      "precision_at_005: 0.1765\n",
      "precision_at_010: 0.1678\n",
      "precision_at_020: 0.1515\n",
      "precision_at_050: 0.1128\n",
      "precision_at_100: 0.0844\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0606\n",
      "recall_at_010: 0.1155\n",
      "recall_at_020: 0.1932\n",
      "recall_at_050: 0.3241\n",
      "recall_at_100: 0.4520\n",
      "## retrieve_eval img2phrase on test ##\n",
      "mean_average_precision: 0.1205\n",
      "mean_reciprocal_rank: 0.3630\n",
      "precision_at_005: 0.1613\n",
      "precision_at_010: 0.1373\n",
      "precision_at_020: 0.1162\n",
      "precision_at_050: 0.0791\n",
      "precision_at_100: 0.0556\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0766\n",
      "recall_at_010: 0.1300\n",
      "recall_at_020: 0.2200\n",
      "recall_at_050: 0.3740\n",
      "recall_at_100: 0.5220\n",
      "\n",
      "epoch100\n",
      "ClipEncoder ready.\n",
      "CLIPP ready\n",
      "1214 remaining imgs\n",
      "1086 remaining imgs\n",
      "958 remaining imgs\n",
      "830 remaining imgs\n",
      "702 remaining imgs\n",
      "574 remaining imgs\n",
      "446 remaining imgs\n",
      "318 remaining imgs\n",
      "190 remaining imgs\n",
      "62 remaining imgs\n",
      "images encoded\n",
      "527 remaining texts\n",
      "399 remaining texts\n",
      "271 remaining texts\n",
      "143 remaining texts\n",
      "15 remaining texts\n",
      "527 remaining texts\n",
      "399 remaining texts\n",
      "271 remaining texts\n",
      "143 remaining texts\n",
      "15 remaining texts\n",
      "phrases encoded\n",
      "torch.Size([655, 512])\n",
      "STAT clipp_scores: shape torch.Size([1342, 655]) device cpu; mean 0.002; min 0.000; max 0.235; std 0.003\n",
      "## retrieve_eval phrase2img on test ##\n",
      "mean_average_precision: 0.1393\n",
      "mean_reciprocal_rank: 0.3160\n",
      "precision_at_005: 0.1795\n",
      "precision_at_010: 0.1716\n",
      "precision_at_020: 0.1531\n",
      "precision_at_050: 0.1136\n",
      "precision_at_100: 0.0839\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0628\n",
      "recall_at_010: 0.1177\n",
      "recall_at_020: 0.1926\n",
      "recall_at_050: 0.3277\n",
      "recall_at_100: 0.4460\n",
      "## retrieve_eval img2phrase on test ##\n",
      "mean_average_precision: 0.1255\n",
      "mean_reciprocal_rank: 0.3824\n",
      "precision_at_005: 0.1729\n",
      "precision_at_010: 0.1469\n",
      "precision_at_020: 0.1175\n",
      "precision_at_050: 0.0797\n",
      "precision_at_100: 0.0555\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0824\n",
      "recall_at_010: 0.1401\n",
      "recall_at_020: 0.2242\n",
      "recall_at_050: 0.3770\n",
      "recall_at_100: 0.5212\n",
      "\n",
      "epoch90\n",
      "ClipEncoder ready.\n",
      "CLIPP ready\n",
      "1214 remaining imgs\n",
      "1086 remaining imgs\n",
      "958 remaining imgs\n",
      "830 remaining imgs\n",
      "702 remaining imgs\n",
      "574 remaining imgs\n",
      "446 remaining imgs\n",
      "318 remaining imgs\n",
      "190 remaining imgs\n",
      "62 remaining imgs\n",
      "images encoded\n",
      "527 remaining texts\n",
      "399 remaining texts\n",
      "271 remaining texts\n",
      "143 remaining texts\n",
      "15 remaining texts\n",
      "527 remaining texts\n",
      "399 remaining texts\n",
      "271 remaining texts\n",
      "143 remaining texts\n",
      "15 remaining texts\n",
      "phrases encoded\n",
      "torch.Size([655, 512])\n",
      "STAT clipp_scores: shape torch.Size([1342, 655]) device cpu; mean 0.002; min 0.000; max 0.268; std 0.003\n",
      "## retrieve_eval phrase2img on test ##\n",
      "mean_average_precision: 0.1378\n",
      "mean_reciprocal_rank: 0.3139\n",
      "precision_at_005: 0.1808\n",
      "precision_at_010: 0.1661\n",
      "precision_at_020: 0.1505\n",
      "precision_at_050: 0.1134\n",
      "precision_at_100: 0.0840\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0621\n",
      "recall_at_010: 0.1162\n",
      "recall_at_020: 0.1907\n",
      "recall_at_050: 0.3228\n",
      "recall_at_100: 0.4428\n",
      "## retrieve_eval img2phrase on test ##\n",
      "mean_average_precision: 0.1207\n",
      "mean_reciprocal_rank: 0.3671\n",
      "precision_at_005: 0.1621\n",
      "precision_at_010: 0.1411\n",
      "precision_at_020: 0.1140\n",
      "precision_at_050: 0.0779\n",
      "precision_at_100: 0.0550\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0772\n",
      "recall_at_010: 0.1344\n",
      "recall_at_020: 0.2173\n",
      "recall_at_050: 0.3681\n",
      "recall_at_100: 0.5168\n",
      "\n",
      "epoch80\n",
      "ClipEncoder ready.\n",
      "CLIPP ready\n",
      "1214 remaining imgs\n",
      "1086 remaining imgs\n",
      "958 remaining imgs\n",
      "830 remaining imgs\n",
      "702 remaining imgs\n",
      "574 remaining imgs\n",
      "446 remaining imgs\n",
      "318 remaining imgs\n",
      "190 remaining imgs\n",
      "62 remaining imgs\n",
      "images encoded\n",
      "527 remaining texts\n",
      "399 remaining texts\n",
      "271 remaining texts\n",
      "143 remaining texts\n",
      "15 remaining texts\n",
      "527 remaining texts\n",
      "399 remaining texts\n",
      "271 remaining texts\n",
      "143 remaining texts\n",
      "15 remaining texts\n",
      "phrases encoded\n",
      "torch.Size([655, 512])\n",
      "STAT clipp_scores: shape torch.Size([1342, 655]) device cpu; mean 0.002; min 0.000; max 0.217; std 0.003\n",
      "## retrieve_eval phrase2img on test ##\n",
      "mean_average_precision: 0.1387\n",
      "mean_reciprocal_rank: 0.3228\n",
      "precision_at_005: 0.1786\n",
      "precision_at_010: 0.1701\n",
      "precision_at_020: 0.1497\n",
      "precision_at_050: 0.1136\n",
      "precision_at_100: 0.0845\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0598\n",
      "recall_at_010: 0.1165\n",
      "recall_at_020: 0.1880\n",
      "recall_at_050: 0.3251\n",
      "recall_at_100: 0.4538\n",
      "## retrieve_eval img2phrase on test ##\n",
      "mean_average_precision: 0.1234\n",
      "mean_reciprocal_rank: 0.3631\n",
      "precision_at_005: 0.1675\n",
      "precision_at_010: 0.1414\n",
      "precision_at_020: 0.1160\n",
      "precision_at_050: 0.0802\n",
      "precision_at_100: 0.0567\n",
      "query_average_precisions: [skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_005: 0.0797\n",
      "recall_at_010: 0.1346\n",
      "recall_at_020: 0.2209\n",
      "recall_at_050: 0.3787\n",
      "recall_at_100: 0.5312\n"
     ]
    }
   ],
   "source": [
    "from utils.clip_plus import ClipPlusEncoder\n",
    "\n",
    "for epoch in range(130, 70, -10):\n",
    "# for epoch in [100]:\n",
    "    print('\\nepoch%d' % epoch)\n",
    "    clipp = ClipPlusEncoder(load_path='output/clipp/models/prompt_lr0.001/clipp_epoch%d.pth' % epoch)\n",
    "    clipp.eval()\n",
    "\n",
    "    img_paths = [os.path.join('data/DTD2/images', img_name) for img_name in dataset.img_splits[split]]\n",
    "    img_vecs = clipp.encode_imgs(img_paths)\n",
    "    print('images encoded')\n",
    "\n",
    "    template = 'An image of %s texture'\n",
    "    phrases = [template % p for p in dataset.phrases]\n",
    "    phrase_vecs = clipp.encode_text_list(phrases)\n",
    "    print('phrases encoded')\n",
    "    print(phrase_vecs.shape)\n",
    "\n",
    "    clipp_scores = img_vecs @ phrase_vecs.T\n",
    "    clipp_scores = clipp_scores.float().softmax(dim=-1).cpu().detach()\n",
    "\n",
    "    print_tensor_stats(clipp_scores, 'clipp_scores')\n",
    "\n",
    "    clipp_p2i = dtd2_eval(mode='phrase2img', match_scores=clipp_scores, dataset=dataset,\n",
    "                         split=split, visualize_path='output/clipp/dtd2_p2i_result')\n",
    "\n",
    "    clipp_i2p = dtd2_eval(mode='img2phrase', match_scores=clipp_scores, dataset=dataset,\n",
    "                     split=split, visualize_path='output/clipp/dtd2_p2i_result')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e5223",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Comparison and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f42eac5a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP\n",
      "image to phrase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/work1/elm/chenyun/Clip/dependencies/DescribingTextures/dtd2/data_api/utils/retrieval_metrics.py:146: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = np.nan_to_num(pred_num * 1.0 / gt_count)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_average_precision: 0.1220\n",
      "mean_reciprocal_rank: 0.4006\n",
      "precision_at_001: 0.2280\n",
      "precision_at_005: 0.1768\n",
      "precision_at_010: 0.1473\n",
      "precision_at_020: 0.1142\n",
      "precision_at_050: 0.0736\n",
      "precision_at_100: 0.0519\n",
      "query_average_precisions: [list, skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_001: 0.0214\n",
      "recall_at_005: 0.0842\n",
      "recall_at_010: 0.1397\n",
      "recall_at_020: 0.2154\n",
      "recall_at_050: 0.3483\n",
      "recall_at_100: 0.4889\n",
      "latex string\n",
      "mean_average_precision & mean_reciprocal_rank & precision_at_005 & precision_at_020 & recall_at_005 & recall_at_020\n",
      "12.20 & 40.06 & 17.68 & 11.42 & 8.42 & 21.54\n",
      "phrase to image\n",
      "mean_average_precision: 0.1274\n",
      "mean_reciprocal_rank: 0.3215\n",
      "precision_at_001: 0.1954\n",
      "precision_at_005: 0.1695\n",
      "precision_at_010: 0.1536\n",
      "precision_at_020: 0.1322\n",
      "precision_at_050: 0.1005\n",
      "precision_at_100: 0.0762\n",
      "query_average_precisions: [list, skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_001: 0.0137\n",
      "recall_at_005: 0.0616\n",
      "recall_at_010: 0.1077\n",
      "recall_at_020: 0.1731\n",
      "recall_at_050: 0.2982\n",
      "recall_at_100: 0.4225\n",
      "latex string\n",
      "mean_average_precision & mean_reciprocal_rank & precision_at_005 & precision_at_020 & recall_at_005 & recall_at_020\n",
      "12.74 & 32.15 & 16.95 & 13.22 & 6.16 & 17.31\n",
      "DTD2_contrastive\n",
      "image to phrase\n",
      "mean_average_precision: 0.3177\n",
      "mean_reciprocal_rank: 0.7412\n",
      "precision_at_001: 0.6080\n",
      "precision_at_005: 0.4170\n",
      "precision_at_010: 0.3256\n",
      "precision_at_020: 0.2360\n",
      "precision_at_050: 0.1375\n",
      "precision_at_100: 0.0847\n",
      "query_average_precisions: [list, skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_001: 0.0598\n",
      "recall_at_005: 0.2017\n",
      "recall_at_010: 0.3135\n",
      "recall_at_020: 0.4504\n",
      "recall_at_050: 0.6488\n",
      "recall_at_100: 0.7934\n",
      "latex string\n",
      "mean_average_precision & mean_reciprocal_rank & precision_at_005 & precision_at_020 & recall_at_005 & recall_at_020\n",
      "31.77 & 74.12 & 41.70 & 23.60 & 20.17 & 45.04\n",
      "phrase to image\n",
      "mean_average_precision: 0.1350\n",
      "mean_reciprocal_rank: 0.3112\n",
      "precision_at_001: 0.1832\n",
      "precision_at_005: 0.1652\n",
      "precision_at_010: 0.1565\n",
      "precision_at_020: 0.1457\n",
      "precision_at_050: 0.1162\n",
      "precision_at_100: 0.0885\n",
      "query_average_precisions: [list, skipped]\n",
      "r_precision: 0.0164\n",
      "recall_at_001: 0.0111\n",
      "recall_at_005: 0.0524\n",
      "recall_at_010: 0.0978\n",
      "recall_at_020: 0.1732\n",
      "recall_at_050: 0.3362\n",
      "recall_at_100: 0.4738\n",
      "latex string\n",
      "mean_average_precision & mean_reciprocal_rank & precision_at_005 & precision_at_020 & recall_at_005 & recall_at_020\n",
      "13.50 & 31.12 & 16.52 & 14.57 & 5.24 & 17.32\n"
     ]
    }
   ],
   "source": [
    "import utils.retrieval_compare as rc\n",
    "importlib.reload(rc)\n",
    "from utils.retrieval_compare import compare_pred_to_html\n",
    "\n",
    "img_html_paths = ['https://www.robots.ox.ac.uk/~vgg/data/dtd/thumbs/%s' % img_name\n",
    "                  for img_name in dataset.img_splits[split]]\n",
    "gt_matrix = dataset.get_img_phrase_match_matrices(split)\n",
    "\n",
    "compare_pred_to_html(img_path_list=img_html_paths,\n",
    "                     phrase_list=dataset.phrases,\n",
    "                     phrase_weight=dataset.phrase_freq,\n",
    "                     gt_matrix=gt_matrix,\n",
    "                     method_score_list=[['CLIP', clip_scores], ['DTD2_contrastive', mdtd2_scores]],\n",
    "                     output_path='output/retrieve_dtd2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283ac75",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
